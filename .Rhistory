library(tidyverse)
library(ggplot2)
library(ggmap)
library(lubridate)
library(rvest)
library(stringr)
library(tidytext)
library(wordcloud)
library(jsonlite)
library(RCurl)
library(XML)
library(RJSONIO)
library(urltools)
options(htmltools.dir.version = FALSE)
#API KEY
key <-"b54a3d8a406247da96456b7bf62ca9ca"
# Tell rtimes what our API key is
Sys.setenv(NYTIMES_AS_KEY = key)
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-10
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
merged_data
min(merged_data$pub_date)
max(merged_data$pub_date)
min(merged_data$pub_date)
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-50
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
num_pages<-30
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
#API KEY
key <-"b54a3d8a406247da96456b7bf62ca9ca"
# Tell rtimes what our API key is
Sys.setenv(NYTIMES_AS_KEY = key)
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-20
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
num_pages<-10
num_pages<-10
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
min(merged_data$pub_date)
#API KEY
key <-"xYByg8wARi7XKcDo9FszpOyWzT0CkkDQ"
# Tell rtimes what our API key is
Sys.setenv(NYTIMES_AS_KEY = key)
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-30
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
num_pages<-10
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
library(tidyverse)
library(ggplot2)
library(ggmap)
library(lubridate)
library(rvest)
library(stringr)
library(tidytext)
library(wordcloud)
library(jsonlite)
library(RCurl)
library(XML)
library(RJSONIO)
library(urltools)
options(htmltools.dir.version = FALSE)
#API KEY
key <-"xYByg8wARi7XKcDo9FszpOyWzT0CkkDQ"
#API KEY
key <-"b54a3d8a406247da96456b7bf62ca9ca"
## make path to .Renviron
file <- file.path(path.expand("~"), ".Renviron")
## save environment variable
cat(key, file = file, append = TRUE, fill = TRUE)
# Tell rtimes what our API key is
Sys.setenv(NYTIMES_AS_KEY = key)
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-10
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
min(merged_data$pub_date)
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-30
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-20
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
num_pages<-15
library(tidyverse)
library(ggplot2)
library(ggmap)
library(lubridate)
library(rvest)
library(stringr)
library(tidytext)
library(wordcloud)
library(jsonlite)
library(RCurl)
library(XML)
library(RJSONIO)
library(urltools)
options(htmltools.dir.version = FALSE)
#API KEY
key <-"b54a3d8a406247da96456b7bf62ca9ca"
## make path to .Renviron
file <- file.path(path.expand("~"), ".Renviron")
## save environment variable
cat(key, file = file, append = TRUE, fill = TRUE)
# Tell rtimes what our API key is
Sys.setenv(NYTIMES_AS_KEY = key)
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-15
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-13
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-10
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
library(tidyverse)
library(ggplot2)
library(ggmap)
library(lubridate)
library(rvest)
library(stringr)
library(tidytext)
library(wordcloud)
library(jsonlite)
library(RCurl)
library(XML)
library(RJSONIO)
library(urltools)
options(htmltools.dir.version = FALSE)
#API KEY
key <-"b54a3d8a406247da96456b7bf62ca9ca"
## make path to .Renviron
file <- file.path(path.expand("~"), ".Renviron")
## save environment variable
cat(key, file = file, append = TRUE, fill = TRUE)
# Tell rtimes what our API key is
Sys.setenv(NYTIMES_AS_KEY = key)
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-10
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-11
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
min(merged_data$pub_date)
dim(merged_data.)
dim(merged_data)
num_pages<-15
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-15
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-14
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-13
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-12
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-11
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
pages<-for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
tryCatch(pages, error=function(e) Sys.sleep(1))
pages
merged_data
num_pages<-10
merged_data<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
url <- param_set(url, "page",k)
pages<-for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
merged_data[(1+k*10):((k+1)*10),]<-docsk
}
merged_data
min(merged_data$pub_date)
resk$response$hits.
resk$response$hits
resk$response$meta$hits
library(tidyverse)
library(ggplot2)
library(ggmap)
library(lubridate)
library(rvest)
library(stringr)
library(tidytext)
library(wordcloud)
library(jsonlite)
library(RCurl)
library(XML)
library(RJSONIO)
library(urltools)
options(htmltools.dir.version = FALSE)
#API KEY
key <-"b54a3d8a406247da96456b7bf62ca9ca"
## make path to .Renviron
file <- file.path(path.expand("~"), ".Renviron")
## save environment variable
cat(key, file = file, append = TRUE, fill = TRUE)
# Tell rtimes what our API key is
Sys.setenv(NYTIMES_AS_KEY = key)
#Trade War
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
num_pages<-10
ny_trade_war<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
pages<-for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
ny_trade_war[(1+k*10):((k+1)*10),]<-docsk
}
#Trade War Opinion Section
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
url <- param_set(url, "section_name", url_encode("Opinion"))
num_pages<-10
ny_trade_opinion<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
pages<-for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
ny_trade_opinion[(1+k*10):((k+1)*10),]<-docsk
}
ny_trade_war
ny_trade_opinion
#Trade War World Section
url <- "http://api.nytimes.com/svc/search/v2/articlesearch.json"
url <- param_set(url, "q", url_encode("Trade War"))
url <- param_set(url, "api-key", url_encode(key))
url <- param_set(url, "section_name", url_encode("World"))
num_pages<-10
ny_trade_world<-tibble(web_url=rep(NA,num_pages*10),snippet=rep(NA,num_pages*10),lead_paragraph=rep(NA,num_pages*10),abstract=rep(NA,num_pages*10),pub_date=rep(NA,num_pages*10))
pages<-for (k in 0:(num_pages-1)){
Sys.sleep(1)
url <- param_set(url, "page",k)
resk <- jsonlite::fromJSON(url)
docsk <- resk$response$docs%>%
select(web_url, snippet, lead_paragraph,abstract,pub_date)
ny_trade_world[(1+k*10):((k+1)*10),]<-docsk
}
ny_trade_world
ny_trade_opinion
ny_trade_world
ny_trade_opinion
ny_trade_world
ny_trade_opinion
ny_trade_world
library(rmd)
library(rvest)
?rvest
write.csv(ny_trade_war, file = "NYT.csv")
knitr::opts_chunk$set(echo = TRUE)
library(httr)
library(rlist)
library(jsonlite)
library(dplyr)
asd<- read.csv("trade_data_us_china.csv")
asd<- read.csv("trade_data_us_china.csv")
asd<- read.csv(file = "trade_data_us_china.csv")
asd<- read.csv(file = "trade_data_us_china.csv")
asd<- read.csv(file = "trade_data_us_china.csv")
asd<- read.csv("trade_data_us_china.csv")
knitr::opts_chunk$set(echo = TRUE)
#file.choose()  #Choose the files that you want
csv<- read.csv(file = "GTAP_tariff_aggregate.csv")
#file.choose()  #Choose the files that you want
csv<- read.csv(file = "GTAP_tariff_aggregate.csv")
read.csv("trade_us_china.csv")
read.csv(file="trade_us_china.csv")
read.csv(file="trade_data_us_china.csv")
